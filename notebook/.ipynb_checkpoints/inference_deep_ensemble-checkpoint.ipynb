{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "972fe547-8f9f-4ef2-baa1-3156c9f4abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torchsummary import summary\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "import nibabel as nib\n",
    "from skimage.transform import resize\n",
    "\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d53feb8-c7e0-44ae-88b2-58485176b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(mean, log_var, targets):\n",
    "    precision = torch.exp(-log_var)\n",
    "    return torch.mean(precision * (targets - mean) ** 2 + log_var)\n",
    "\n",
    "class CNN_Trainer():\n",
    "    def __init__(self, model, results_folder, dataloader_train, dataloader_valid, dataloader_test, epochs, optimizer, scheduler):\n",
    "        super(CNN_Trainer, self).__init__()\n",
    "\n",
    "        self.models = [model]\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_valid = dataloader_valid\n",
    "        self.dataloader_test = dataloader_test\n",
    "        self.epochs = epochs\n",
    "        self.optimizers = [optimizer]\n",
    "        self.schedulers = [scheduler]\n",
    "        self.results_folder = Path(results_folder)\n",
    "        self.results_folder.mkdir(exist_ok=True)\n",
    "        self.valid_loss_min = np.Inf  # 검증 손실의 최소값 초기화\n",
    "\n",
    "        # wandb.watch(model, log=\"all\")  # WandB에 모델 로깅 시작\n",
    "\n",
    "    def evaluate_uncertainty(self):\n",
    "        \"\"\"앙상블의 모든 모델에 대해 평균과 분산 계산 및 로깅\"\"\"\n",
    "        ensemble_means = []\n",
    "        ensemble_variances = []\n",
    "        with torch.no_grad():\n",
    "            for model in self.models:\n",
    "                model.eval()\n",
    "                means = []\n",
    "                variances = []\n",
    "                for inputs, _ in self.dataloader_valid:\n",
    "                    inputs = inputs.cuda()\n",
    "                    outputs = model(inputs)\n",
    "                    mean = outputs[:, 0]\n",
    "                    log_var = outputs[:, 1]\n",
    "                    variance = torch.exp(log_var)\n",
    "                    \n",
    "                    means.append(mean)\n",
    "                    variances.append(variance)\n",
    "                \n",
    "                ensemble_means.append(torch.cat(means, dim=0))\n",
    "                ensemble_variances.append(torch.cat(variances, dim=0))\n",
    "\n",
    "        # 앙상블 평균과 분산 계산\n",
    "        final_mean = torch.mean(torch.stack(ensemble_means), dim=0)\n",
    "        final_variance = torch.mean(torch.stack(ensemble_variances), dim=0) + torch.var(torch.stack(ensemble_means), dim=0)\n",
    "        \n",
    "        # WandB에 로깅\n",
    "        # wandb.log({\n",
    "        #     \"Ensemble Mean Uncertainty\": final_mean.mean().item(),\n",
    "        #     \"Ensemble Variance Uncertainty\": final_variance.mean().item()\n",
    "        # })\n",
    "\n",
    "\n",
    "    def add_model(self, new_model, optimizer, scheduler):\n",
    "        \"\"\"새로운 모델을 앙상블에 추가하고 불확실성 평가\"\"\"\n",
    "        self.models.append(new_model)\n",
    "        self.optimizers.append(optimizer)\n",
    "        self.schedulers.append(scheduler)\n",
    "        # 앙상블에 모델 추가 후 불확실성 평가\n",
    "        self.evaluate_uncertainty()\n",
    "\n",
    "\n",
    "    def validate(self, model):\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        mae_sum = 0.0\n",
    "        mean_sum = 0.0  # 평균의 합계\n",
    "        variance_sum = 0.0  # 분산의 합계\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.dataloader_valid:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                outputs = model(inputs)\n",
    "                mean = outputs[:, 0]\n",
    "                log_var = outputs[:, 1]\n",
    "                variance = torch.exp(log_var)  # 로그 분산을 분산으로 변환\n",
    "\n",
    "                loss = nll_loss(mean, log_var, targets)\n",
    "                val_loss_sum += loss.item() * inputs.size(0)\n",
    "                mae = torch.abs(mean - targets).mean()\n",
    "                mae_sum += mae.item() * inputs.size(0)\n",
    "\n",
    "                mean_sum += mean.sum().item()\n",
    "                variance_sum += variance.sum().item()\n",
    "                count += inputs.size(0)\n",
    "\n",
    "        val_loss_avg = val_loss_sum / count\n",
    "        mae_avg = mae_sum / count\n",
    "        mean_avg = mean_sum / count  # 평균의 평균\n",
    "        variance_avg = variance_sum / count  # 분산의 평균\n",
    "\n",
    "        return val_loss_avg, mae_avg, mean_avg, variance_avg\n",
    "\n",
    "\n",
    "\n",
    "    def train_single_model(self, model_idx):\n",
    "        model = self.models[model_idx]\n",
    "        optimizer = self.optimizers[model_idx]\n",
    "        scheduler = self.schedulers[model_idx]\n",
    "\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            model.train()\n",
    "            train_loss_sum = 0.0\n",
    "            train_mean_sum = 0.0  # 훈련 데이터 평균의 합계\n",
    "            train_variance_sum = 0.0  # 훈련 데이터 분산의 합계\n",
    "            count = 0\n",
    "\n",
    "            for inputs, targets in self.dataloader_train:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                mean = outputs[:, 0]\n",
    "                log_var = outputs[:, 1]\n",
    "                variance = torch.exp(log_var)  # 로그 분산을 분산으로 변환\n",
    "\n",
    "                loss = nll_loss(mean, log_var, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss_sum += loss.item() * inputs.size(0)\n",
    "                train_mean_sum += mean.sum().item()\n",
    "                train_variance_sum += variance.sum().item()\n",
    "                count += inputs.size(0)\n",
    "\n",
    "            train_loss_avg = train_loss_sum / count\n",
    "            train_mean_avg = train_mean_sum / count  # 훈련 데이터 평균의 평균\n",
    "            train_variance_avg = train_variance_sum / count  # 훈련 데이터 분산의 평균\n",
    "\n",
    "            val_loss_avg, val_mae_avg, val_mean_avg, val_variance_avg = self.validate(model)\n",
    "            scheduler.step(val_loss_avg)  # Scheduler 업데이트\n",
    "\n",
    "            self.evaluate_uncertainty()\n",
    "\n",
    "            # # WandB 로깅\n",
    "            # wandb.log({\n",
    "            #     f\"Epoch\": epoch,\n",
    "            #     f\"Train Loss Model {model_idx+1}\": train_loss_avg,\n",
    "            #     f\"Validation Loss Model {model_idx+1}\": val_loss_avg,\n",
    "            #     f\"Validation MAE Model {model_idx+1}\": val_mae_avg,\n",
    "            #     f\"Train Mean Model {model_idx+1}\": train_mean_avg,\n",
    "            #     f\"Train Variance Model {model_idx+1}\": train_variance_avg,\n",
    "            #     f\"Validation Mean Model {model_idx+1}\": val_mean_avg,\n",
    "            #     f\"Validation Variance Model {model_idx+1}\": val_variance_avg,\n",
    "            # })\n",
    "\n",
    "            # print(f\"Epoch [{epoch+1}/{self.epochs}], Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}, Val MAE: {val_mae_avg:.4f}, Train Mean: {train_mean_avg:.4f}, Train Variance: {train_variance_avg:.4f}\")\n",
    "\n",
    "\n",
    "    def train_ensemble(self):\n",
    "        for model_idx in range(len(self.models)):\n",
    "            self.train_single_model(model_idx)\n",
    "            self.save(model_idx)\n",
    "        self.evaluate_uncertainty()\n",
    "\n",
    "\n",
    "    def test_ensemble(self):\n",
    "        \"\"\"앙상블 모델로 테스트 데이터에 대한 예측을 수행하고 평균, 분산, MAE를 계산\"\"\"\n",
    "        ensemble_predictions = []\n",
    "        ensemble_variances = []\n",
    "        mae_list = []\n",
    "        with torch.no_grad():\n",
    "            for model_idx, model in enumerate(self.models):\n",
    "                model.eval()  # 모델을 평가 모드로 설정\n",
    "                predictions = []\n",
    "                variances = []\n",
    "                total_mae = 0\n",
    "                count = 0\n",
    "                for inputs, targets in self.dataloader_test:\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    outputs = model(inputs)\n",
    "                    mean = outputs[:, 0]\n",
    "                    log_var = outputs[:, 1]\n",
    "                    variance = torch.exp(log_var)  # 로그 분산을 분산으로 변환\n",
    "\n",
    "                    mae = torch.abs(mean - targets).mean().item()\n",
    "                    total_mae += mae * inputs.size(0)\n",
    "                    count += inputs.size(0)\n",
    "                    \n",
    "                    predictions.append(mean)\n",
    "                    variances.append(variance)\n",
    "\n",
    "                model_mae = total_mae / count\n",
    "                mae_list.append(model_mae)\n",
    "                \n",
    "                # 모델별 예측 수행 및 저장\n",
    "                model_predictions = torch.cat(predictions, dim=0)\n",
    "                model_variances = torch.cat(variances, dim=0)\n",
    "                ensemble_predictions.append(model_predictions)\n",
    "                ensemble_variances.append(model_variances)\n",
    "\n",
    "            # 앙상블 예측을 위한 평균 및 분산 계산\n",
    "            final_prediction = torch.mean(torch.stack(ensemble_predictions), dim=0)\n",
    "            final_variance = torch.mean(torch.stack(ensemble_variances), dim=0) + torch.var(torch.stack(ensemble_predictions), dim=0)\n",
    "            \n",
    "            # 앙상블에 대한 MAE 계산\n",
    "            ensemble_mae = sum(mae_list) / len(mae_list)\n",
    "\n",
    "            # # WandB에 테스트 결과 로깅\n",
    "            # wandb.log({\n",
    "            #     \"Test Mean\": final_prediction.mean().item(),\n",
    "            #     \"Test Variance\": final_variance.mean().item(),\n",
    "            #     \"Test MAE\": ensemble_mae\n",
    "            # })\n",
    "\n",
    "            return final_prediction, final_variance, ensemble_mae\n",
    "\n",
    "\n",
    "\n",
    "    def predict_with_features(self, model, features):\n",
    "        predictions = []\n",
    "        for feature in features:\n",
    "            prediction = model.module.fc(feature.unsqueeze(0))  # feature.unsqueeze(0)는 배치 차원을 추가\n",
    "            predictions.append(prediction)\n",
    "        return torch.cat(predictions, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, model_idx):\n",
    "        model_save_path = f\"{self.results_folder}/model_{model_idx}.pth\"\n",
    "        torch.save({\"epoch\": model_idx+1, \n",
    "                    \"state_dict\": self.model.state_dict(), \n",
    "                    \"optimizer\" : self.optimizer.state_dict(),  \n",
    "                    \"train_mae_list\": self.train_mae_list,\n",
    "                    \"valid_mae_list\": self.valid_mae_list},  \n",
    "                    model_save_path)\n",
    "        print(f\"Model {model_idx+1} saved to {model_save_path}\")\n",
    "\n",
    "    def load(self, checkpoint):\n",
    "        self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        self.epoch = checkpoint[\"epoch\"]  # Get the epoch directly from the checkpoint\n",
    "        self.train_mse_list = checkpoint.get(\"train_mse_list\", [])\n",
    "        self.train_mae_list = checkpoint.get(\"train_mae_list\", [])\n",
    "        self.valid_mse_list = checkpoint.get(\"valid_mse_list\", [])\n",
    "        self.valid_mae_list = checkpoint.get(\"valid_mae_list\", [])\n",
    "\n",
    "    \n",
    "    def save_features(self, milestone):\n",
    "        all_features = torch.cat(self.feature_list, dim=0)\n",
    "        # features 디렉토리 경로 설정\n",
    "        features_folder = Path(self.results_folder) / 'feat'\n",
    "        features_folder.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(all_features, f\"{features_folder}/cv-{self.cv_num}-features-{milestone+1}.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# Model define\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv3d) or isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "            \n",
    "# Define 3D_CNN model class\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=(3,3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.Conv3d(32, 32, kernel_size=(3,3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout3d(0.2),\n",
    "            nn.MaxPool3d(kernel_size=(2,2,2))\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.Conv3d(64, 64, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout3d(0.2),\n",
    "            nn.MaxPool3d(kernel_size=(2,2,2))\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv3d(64, 64, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.Conv3d(64, 64, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.MaxPool3d(2)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv3d(64, 64, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.Conv3d(64, 64, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.Conv3d(64, 64, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.MaxPool3d(2)\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv3d(64, 96, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(96),\n",
    "            nn.Conv3d(96, 96, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(96),\n",
    "            nn.Conv3d(96, 96, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(96),\n",
    "            nn.MaxPool3d(2)\n",
    "        )\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv3d(96, 96, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(96),\n",
    "            nn.Conv3d(96, 96, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(96),\n",
    "            nn.Conv3d(96, 96, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(96),\n",
    "            nn.MaxPool3d(2)\n",
    "        )\n",
    "\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv3d(96, 96, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(96),\n",
    "            nn.Conv3d(96, 96, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(96),\n",
    "            nn.Conv3d(96, 96, kernel_size=(3,3,3), stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm3d(96)\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # 평균과 로그 분산을 출력하기 위한 fully connected 레이어\n",
    "        # 출력 차원을 2로 설정 (하나는 평균, 하나는 로그 분산)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768, 96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)  # 출력 차원을 2로 변경\n",
    "        )\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.fc(x)\n",
    "        # 출력이 평균과 로그 분산으로 구성\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d80c995-cc79-4440-9f09-ad65dfef7b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectID</th>\n",
       "      <th>imgs</th>\n",
       "      <th>mask</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>caudate</th>\n",
       "      <th>caudate_mask</th>\n",
       "      <th>cerebellum</th>\n",
       "      <th>cerebellum_mask</th>\n",
       "      <th>frontal_lobe</th>\n",
       "      <th>...</th>\n",
       "      <th>occipital_lobe</th>\n",
       "      <th>occipital_lobe_mask</th>\n",
       "      <th>parietal_lobe</th>\n",
       "      <th>parietal_lobe_mask</th>\n",
       "      <th>putamen</th>\n",
       "      <th>putamen_mask</th>\n",
       "      <th>temporal_lobe</th>\n",
       "      <th>temporal_lobe_mask</th>\n",
       "      <th>thalamus</th>\n",
       "      <th>thalamus_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-002S0295X20110602</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>M</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-002S0413X20060519</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>76.5</td>\n",
       "      <td>F</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-002S0559X20060627</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>79.5</td>\n",
       "      <td>M</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-002S0685X20110708</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>94.7</td>\n",
       "      <td>F</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-002S1261X20070227</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>71.2</td>\n",
       "      <td>F</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "      <td>/media/leelabsg-storage1/yein/research/data/ad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               subjectID                                               imgs  \\\n",
       "0  sub-002S0295X20110602  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  sub-002S0413X20060519  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  sub-002S0559X20060627  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  sub-002S0685X20110708  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  sub-002S1261X20070227  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                                mask   age gender  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...  90.0      M   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...  76.5      F   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...  79.5      M   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...  94.7      F   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...  71.2      F   \n",
       "\n",
       "                                             caudate  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                        caudate_mask  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                          cerebellum  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                     cerebellum_mask  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                        frontal_lobe  ...  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...  ...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...  ...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...  ...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...  ...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...  ...   \n",
       "\n",
       "                                      occipital_lobe  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                 occipital_lobe_mask  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                       parietal_lobe  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                  parietal_lobe_mask  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                             putamen  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                        putamen_mask  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                       temporal_lobe  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                  temporal_lobe_mask  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                            thalamus  \\\n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...   \n",
       "\n",
       "                                       thalamus_mask  \n",
       "0  /media/leelabsg-storage1/yein/research/data/ad...  \n",
       "1  /media/leelabsg-storage1/yein/research/data/ad...  \n",
       "2  /media/leelabsg-storage1/yein/research/data/ad...  \n",
       "3  /media/leelabsg-storage1/yein/research/data/ad...  \n",
       "4  /media/leelabsg-storage1/yein/research/data/ad...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv('/media/leelabsg-storage1/yein/research/BAE/RegionBAE/data/adni_cn_region.csv')\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7864e2b3-bad2-4458-98fd-0927630a800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 630\n",
    "regions = {0: 'imgs', 1: 'caudate', 2: 'cerebellum', 3: 'frontal_lobe', 4: 'insula', 5: 'occipital_lobe', 6: 'parietal_lobe', 7: 'putamen', 8: 'temporal_lobe', 9: 'thalamus'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aaf6e7a8-12b8-4f2f-a9cb-39dff6924614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== imgs ====================\n",
      "Model:  /media/leelabsg-storage1/yein/research/model/region_BAE/deep_ensemble/imgs/model_0.pth\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CNN_Trainer' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;124m\"\u001b[39m, best_model_path)\n\u001b[1;32m     47\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(best_model_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m predictions, targets \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest()\n",
      "Cell \u001b[0;32mIn[38], line 233\u001b[0m, in \u001b[0;36mCNN_Trainer.load\u001b[0;34m(self, checkpoint)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, checkpoint):\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Get the epoch directly from the checkpoint\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CNN_Trainer' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "ROI = regions[0]\n",
    "print(\"=\" * 20, ROI, \"=\" * 20)\n",
    "dataset_indices = list(dataset_df.index)[:DATA_SIZE]\n",
    "test_dataset = Region_Dataset(dataset_df, dataset_indices, ROI)\n",
    "dataloader_test = DataLoader(test_dataset, \n",
    "                            batch_size=1, \n",
    "                            sampler=SequentialSampler(test_dataset),\n",
    "                            collate_fn=test_dataset.collate_fn,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=2)\n",
    "\n",
    "# hypterparameters\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 2\n",
    "RESULTS_FOLDER = './test'\n",
    "INPUT_SIZE = (1, 128, 128, 128)\n",
    "LEARNING_RATE = 1e-6\n",
    "N_WORKERS = 2\n",
    "\n",
    "# Initialize your model (Make sure it's the same architecture as the one you trained)\n",
    "model = CNN(in_channels=1).cuda()  \n",
    "# Put your model on the GPU\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Define your optimizer and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "t_0 = int(DATA_SIZE * 0.75 // BATCH_SIZE // 6)\n",
    "scheduler = CustomCosineAnnealingWarmUpRestarts(optimizer,T_0= t_0, T_up=10, T_mult=2, eta_max=1e-3, gamma=0.5)\n",
    "\n",
    "# Loss function\n",
    "mse_criterion = torch.nn.MSELoss()\n",
    "mae_criterion = torch.nn.L1Loss()\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = CNN_Trainer(model, RESULTS_FOLDER, None, None, dataloader_test, EPOCHS, optimizer, scheduler)\n",
    "\n",
    "model_root = '/media/leelabsg-storage1/yein/research/model/region_BAE/deep_ensemble'\n",
    "all_model_paths = glob.glob(model_root + f'/{ROI}/*')\n",
    "best_model_path = all_model_paths[0]\n",
    "# max_epoch = 0\n",
    "# for path in all_model_paths:\n",
    "#     epoch = int(path.split('.')[0][-1])\n",
    "#     if epoch > max_epoch:\n",
    "#         max_epoch = epoch\n",
    "#         best_model_path = path\n",
    "print(\"Model: \", best_model_path)\n",
    "checkpoint = torch.load(best_model_path, map_location='cuda')\n",
    "trainer.load(checkpoint)\n",
    "predictions, targets = trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38187506-318a-46b6-becd-47b6dcb1dc51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc0ab5-7827-4151-b4d5-3fb447bb5e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62456db2-8c07-482f-a095-89ab6e21f80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== imgs ====================\n",
      "Model:  /media/leelabsg-storage1/yein/research/model/region/imgs/cv-0-97.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 630/630 [00:52<00:00, 12.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mse loss = 90.611 / test mae loss = 8.145\n",
      "==================== caudate ====================\n",
      "Model:  /media/leelabsg-storage1/yein/research/model/region/caudate/cv-0-30.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 630/630 [00:53<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mse loss = 134.677 / test mae loss = 9.636\n",
      "==================== cerebellum ====================\n",
      "Model:  /media/leelabsg-storage1/yein/research/model/region/cerebellum/cv-0-56.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 630/630 [01:22<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mse loss = 102.942 / test mae loss = 8.416\n",
      "==================== frontal_lobe ====================\n",
      "Model:  /media/leelabsg-storage1/yein/research/model/region/frontal_lobe/cv-0-91.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 630/630 [00:47<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mse loss = 136.601 / test mae loss = 9.897\n",
      "==================== insula ====================\n",
      "Model:  /media/leelabsg-storage1/yein/research/model/region/insula/cv-0-34.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                       | 411/630 [00:58<00:31,  7.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(best_model_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m trainer\u001b[38;5;241m.\u001b[39mload(checkpoint)\n\u001b[0;32m---> 60\u001b[0m predictions, targets \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 283\u001b[0m, in \u001b[0;36mCNN_Trainer.test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    282\u001b[0m     test_mse_sum, test_mae_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, (\u001b[38;5;28minput\u001b[39m, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader_test)):\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mcuda(non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    285\u001b[0m         target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/vx/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/vx/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/micromamba/envs/vx/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/vx/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/micromamba/envs/vx/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/vx/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/micromamba/envs/vx/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in regions.keys():\n",
    "    ROI = regions[i]\n",
    "    print(\"=\" * 20, ROI, \"=\" * 20)\n",
    "    dataset_indices = list(dataset_df.index)[:DATA_SIZE]\n",
    "    test_dataset = Region_Dataset(dataset_df, dataset_indices, ROI)\n",
    "    dataloader_test = DataLoader(test_dataset, \n",
    "                                batch_size=1, \n",
    "                                sampler=SequentialSampler(test_dataset),\n",
    "                                collate_fn=test_dataset.collate_fn,\n",
    "                                pin_memory=True,\n",
    "                                num_workers=2)\n",
    "    \n",
    "    # hypterparameters\n",
    "    BATCH_SIZE = 8\n",
    "    EPOCHS = 2\n",
    "    RESULTS_FOLDER = './test'\n",
    "    INPUT_SIZE = (1, 128, 128, 128)\n",
    "    LEARNING_RATE = 1e-6\n",
    "    N_WORKERS = 2\n",
    "    \n",
    "    # Initialize your model (Make sure it's the same architecture as the one you trained)\n",
    "    model = CNN(in_channels=1).cuda()\n",
    "    model.apply(initialize_weights)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = lr.CustomCosineAnnealingWarmUpRestarts(optimizer,T_0= t_0, T_up=10, T_mult=2, eta_max=1e-3, gamma=0.5)\n",
    "    \n",
    "    # 훈련 클래스 초기화\n",
    "\n",
    "    trainer = CNN_Trainer(model, OUTPUT, dataloader_train, dataloader_valid, None, EPOCHS, optimizer, scheduler, model_load=MODEL_LOAD)\n",
    "    \n",
    "\n",
    "    model_root = '/media/leelabsg-storage1/yein/research/model/region_BAE/deep_ensemble'\n",
    "    all_model_paths = glob.glob(model_root + f'/{ROI}/*')\n",
    "    best_model_path = ''\n",
    "    max_epoch = 0\n",
    "    for path in all_model_paths:\n",
    "        epoch = int(path.split('/')[-1].split('-')[2].split('.')[0])\n",
    "        if epoch > max_epoch:\n",
    "            max_epoch = epoch\n",
    "            best_model_path = path\n",
    "    print(\"Model: \", best_model_path)\n",
    "    checkpoint = torch.load(best_model_path, map_location='cuda')\n",
    "    trainer.load(checkpoint)\n",
    "    predictions, targets = trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd5627-b34e-4cbd-b31c-1f02156c5fda",
   "metadata": {},
   "source": [
    "# 각 region 별 volume 값 계산하여 mean, variance 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83da2eb-5753-483a-9c05-f998d3ff91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ants\n",
    "from helpers import *\n",
    "import gc\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e3824b-f4cb-4adc-a750-34b7556b1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_wise_volumes = {\n",
    "    'age': dataset_df['age'].to_list(),\n",
    "    'gender': dataset_df['gender'].to_list()\n",
    "}\n",
    "\n",
    "regions = {0: 'imgs', 1: 'caudate', 2: 'cerebellum', 3: 'frontal_lobe', 4: 'insula', 5: 'occipital_lobe', 6: 'parietal_lobe', 7: 'putamen', 8: 'temporal_lobe', 9: 'thalamus'}\n",
    "for region in regions.values():\n",
    "    region_wise_volumes[region] = []\n",
    "\n",
    "# 초기 'age'와 'gender' 리스트에 대한 길이 검증\n",
    "assert len(region_wise_volumes['age']) == len(dataset_df), \"Age list length mismatch\"\n",
    "assert len(region_wise_volumes['gender']) == len(dataset_df), \"Gender list length mismatch\"\n",
    "\n",
    "for i in range(len(dataset_df)): # 대신 작은 숫자로 테스트\n",
    "    for ROI in regions.values():\n",
    "        try:\n",
    "            img_path = dataset_df.iloc[i][ROI]  # 올바른 경로로 접근\n",
    "            img = ants.image_read(img_path)  # 이미지 로드\n",
    "            img_arr = img.numpy()  # ANTs 이미지 객체를 NumPy 배열로 변환\n",
    "            nonzero_count = np.count_nonzero(img_arr)  # 0이 아닌 픽셀의 개수 계산\n",
    "            region_wise_volumes[ROI].append(nonzero_count)  # 결과 저장\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ROI} for row {i}: {e}\")\n",
    "            region_wise_volumes[ROI].append(None)  # 오류 발생 시 None 추가\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976afe12-ed7b-49f0-b121-4464b29015b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "region_wise_volumes_df = pd.DataFrame(region_wise_volumes)\n",
    "region_wise_volumes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc7dd6-6d19-489b-bdfd-915c8edd2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# region_wise_volumes_df는 이미 생성된 데이터프레임 변수라고 가정\n",
    "\n",
    "# 각 영역 볼륨의 평균과 분산 계산\n",
    "volume_stats = {}\n",
    "regions = ['caudate', 'cerebellum', 'frontal_lobe', 'insula', 'occipital_lobe', 'parietal_lobe', 'putamen', 'temporal_lobe', 'thalamus']\n",
    "\n",
    "for region in regions:\n",
    "    volume_stats[region] = {\n",
    "        'mean': region_wise_volumes_df[region].mean(),\n",
    "        'variance': region_wise_volumes_df[region].var(),\n",
    "        'mean_ratio': region_wise_volumes_df[region].mean() / region_wise_volumes_df['imgs'].mean()\n",
    "    }\n",
    "\n",
    "# 결과를 DataFrame으로 변환하여 표시\n",
    "volume_stats_df = pd.DataFrame(volume_stats).T\n",
    "volume_stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c57c3-b13e-4911-9963-36888d3705ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# regions 딕셔너리 정의가 이전과 동일하다고 가정합니다.\n",
    "regions = {0: 'imgs', 1: 'caudate', 2: 'cerebellum', 3: 'frontal_lobe', 4: 'insula', \n",
    "           5: 'occipital_lobe', 6: 'parietal_lobe', 7: 'putamen', \n",
    "           8: 'temporal_lobe', 9: 'thalamus'}\n",
    "\n",
    "# 각 영역에 대한 히스토그램을 그립니다.\n",
    "plt.figure(figsize=(10, 8))  # 그래프의 크기를 설정합니다.\n",
    "\n",
    "# 각 영역 별로 히스토그램을 그리는 반복문\n",
    "for region in regions.values():\n",
    "    region_wise_volumes_df[region].hist(alpha=0.5, label=region)  # alpha는 투명도를 조절합니다.\n",
    "\n",
    "plt.legend()  # 범례를 표시합니다.\n",
    "plt.xlabel('Volume')  # x축 라벨\n",
    "plt.ylabel('Frequency')  # y축 라벨\n",
    "plt.title('Histogram of Volumes by Region')  # 그래프 제목\n",
    "plt.show()  # 그래프를 화면에 표시합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7970f98c-fdb0-47c2-92a2-91b7a9d001ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102d035-d245-47be-ab19-29efe697d246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17472e-a8a0-4630-92f3-c5bb743d2838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0caff-d97d-442c-8df7-b411232e27fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0b3a5-6cb5-4951-b5d6-49ae3d4eb879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
